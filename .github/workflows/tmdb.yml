name: TMDB

on:
  schedule:
    - cron: "0 8 * * *"
  workflow_dispatch:

jobs:
  mask:
    runs-on: ubuntu-latest
    concurrency: tmdb_mask_${{ matrix.type }}

    strategy:
      fail-fast: false
      matrix:
        include:
          - type: "movie"
            export_name: "movie_ids"
          - type: "tv"
            export_name: "tv_series_ids"
          - type: "person"
            export_name: "person_ids"

    steps:
      - uses: actions/checkout@v3

      - name: Set up Python 3.10
        uses: actions/setup-python@v4
        with:
          python-version: "3.10"
          cache: "pip"

      - name: Install dependencies
        run: |
          pip install -r <(grep -E 'numpy|pyarrow' requirements.txt)

      - name: Generate export filename
        id: export_filename
        run: |
          TODAY=$(date --date=today +'%m_%d_%Y')
          YESTERDAY=$(date --date=yesterday +'%m_%d_%Y')
          echo "today_filename=${EXPORT_NAME}_${TODAY}.json.gz" >>"$GITHUB_OUTPUT"
          echo "yesterday_filename=${EXPORT_NAME}_${YESTERDAY}.json.gz" >>"$GITHUB_OUTPUT"
        env:
          EXPORT_NAME: ${{ matrix.export_name }}

      - name: Download today's export
        id: wget_today_export
        continue-on-error: true
        run: |
          wget -O "$EXPORT_NAME.json.gz" "http://files.tmdb.org/p/exports/$FILENAME"
        env:
          FILENAME: ${{ steps.export_filename.outputs.today_filename }}
          EXPORT_NAME: ${{ matrix.export_name }}

      - name: Download yesterdays's export
        id: wget_yesterday_export
        if: steps.wget_today_export.outcome == 'failure'
        run: |
          wget -O "$EXPORT_NAME.json.gz" "http://files.tmdb.org/p/exports/$FILENAME"
        env:
          FILENAME: ${{ steps.export_filename.outputs.yesterday_filename }}
          EXPORT_NAME: ${{ matrix.export_name }}

      - name: Run script
        shell: python
        run: |
          import os

          import numpy as np
          import pyarrow as pa
          import pyarrow.feather as feather
          from pyarrow import json

          export_name = os.environ["EXPORT_NAME"]
          table = json.read_json(f"{export_name}.json.gz")
          valid_ids = table["id"].to_numpy()
          size = valid_ids.max() + 1
          mask = np.ones(size, bool)
          mask[valid_ids] = 0
          null_count = np.sum(mask)
          print(f"Null count: {null_count}/{size}")
          table = pa.Table.from_arrays([mask], names=["null"])
          feather.write_feather(table, "mask.arrow")
        env:
          EXPORT_NAME: ${{ matrix.export_name }}

      - name: Upload artifacts
        uses: actions/upload-artifact@v3
        with:
          name: ${{ matrix.type }}-mask-export
          path: |
            *.arrow
          retention-days: 3

      - name: Upload to S3
        run: |
          aws s3 cp "mask.arrow" "s3://$BUCKET_NAME/tmdb/$TYPE/mask.arrow"
        env:
          AWS_ACCESS_KEY_ID: ${{ secrets.AWS_ACCESS_KEY_ID }}
          AWS_SECRET_ACCESS_KEY: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          BUCKET_NAME: wikidatabots
          AWS_DEFAULT_REGION: us-east-1
          TYPE: ${{ matrix.type }}

  external_ids:
    runs-on: ubuntu-latest
    concurrency: tmdb_external_ids_${{ matrix.type }}

    strategy:
      fail-fast: false
      matrix:
        include:
          - type: "movie"
          - type: "tv"
          - type: "person"

    steps:
      - uses: actions/checkout@v3

      - name: Set up Python 3.10
        uses: actions/setup-python@v4
        with:
          python-version: "3.10"
          cache: "pip"

      - name: Install dependencies
        run: |
          pip install -r requirements.txt

      - name: Download from S3
        run: |
          aws s3 cp "s3://$BUCKET_NAME/tmdb/$TYPE/external_ids.arrow" "external_ids.arrow"
        env:
          AWS_ACCESS_KEY_ID: ${{ secrets.AWS_ACCESS_KEY_ID }}
          AWS_SECRET_ACCESS_KEY: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          BUCKET_NAME: wikidatabots
          AWS_DEFAULT_REGION: us-east-1
          TYPE: ${{ matrix.type }}

      - name: Print stats
        shell: python
        run: |
          import os

          import numpy as np
          import pyarrow.feather as feather

          table = feather.read_table("external_ids.arrow")

          col = table.column("imdb_id").to_numpy()
          print(f"imdb_id: {np.count_nonzero(col)}/{col.size}")

          if os.environ["TYPE"] == "tv":
              col = table.column("tvdb_id").to_numpy()
              print(f"tvdb_id: {np.count_nonzero(col)}/{col.size}")

          col = table.column("retrieved_at").to_numpy()
          print(f"timestamps: {np.count_nonzero(~np.isnan(col))}/{col.size}")
        env:
          TYPE: ${{ matrix.type }}

      - name: Run script
        run: |
          python tmdb_update_external_ids.py "$TYPE" "external_ids.arrow"
        env:
          TMDB_API_KEY: ${{ secrets.TMDB_API_KEY }}
          TYPE: ${{ matrix.type }}

      - name: Print stats
        shell: python
        run: |
          import os

          import numpy as np
          import pyarrow.feather as feather

          table = feather.read_table("external_ids.arrow")

          col = table.column("imdb_id").to_numpy()
          print(f"imdb_id: {np.count_nonzero(col)}/{col.size}")

          if os.environ["TYPE"] == "tv":
              col = table.column("tvdb_id").to_numpy()
              print(f"tvdb_id: {np.count_nonzero(col)}/{col.size}")

          col = table.column("retrieved_at").to_numpy()
          print(f"timestamps: {np.count_nonzero(~np.isnan(col))}/{col.size}")
        env:
          TYPE: ${{ matrix.type }}

      - name: Upload artifacts
        uses: actions/upload-artifact@v3
        with:
          name: ${{ matrix.type }}-external_ids-export
          path: |
            *.arrow
          retention-days: 3

      - name: Upload to S3
        run: |
          aws s3 cp "external_ids.arrow" "s3://$BUCKET_NAME/tmdb/$TYPE/external_ids.arrow"
        env:
          AWS_ACCESS_KEY_ID: ${{ secrets.AWS_ACCESS_KEY_ID }}
          AWS_SECRET_ACCESS_KEY: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          BUCKET_NAME: wikidatabots
          AWS_DEFAULT_REGION: us-east-1
          TYPE: ${{ matrix.type }}
