name: TMDB ETL

on:
  schedule:
    - cron: "0 8 * * *"
  workflow_dispatch:

jobs:
  export:
    runs-on: ubuntu-latest
    concurrency: tmdb_export_${{ matrix.type }}

    strategy:
      fail-fast: false
      matrix:
        include:
          - type: "movie"
            export_name: "movie_ids"
          - type: "tv"
            export_name: "tv_series_ids"
          - type: "person"
            export_name: "person_ids"

    steps:
      - uses: actions/checkout@v3

      - name: Set up Python 3.10
        uses: actions/setup-python@v4
        with:
          python-version: "3.10"
          cache: "pip"

      - name: Install dependencies
        run: |
          pip install -r <(grep -E 'numpy|pyarrow' requirements.txt)

      - name: Generate export filename
        id: export_filename
        run: |
          TODAY=$(date --date=today +'%m_%d_%Y')
          YESTERDAY=$(date --date=yesterday +'%m_%d_%Y')
          echo "today_filename=${EXPORT_NAME}_${TODAY}.json.gz" >>"$GITHUB_OUTPUT"
          echo "yesterday_filename=${EXPORT_NAME}_${YESTERDAY}.json.gz" >>"$GITHUB_OUTPUT"
        env:
          EXPORT_NAME: ${{ matrix.export_name }}

      - name: Download today's export
        id: curl_today_export
        continue-on-error: true
        run: |
          curl "http://files.tmdb.org/p/exports/$FILENAME" | gunzip >"$EXPORT_NAME.json"
        env:
          FILENAME: ${{ steps.export_filename.outputs.today_filename }}
          EXPORT_NAME: ${{ matrix.export_name }}

      - name: Download yesterdays's export
        id: curl_yesterday_export
        if: steps.curl_today_export.outcome == 'failure'
        run: |
          curl "http://files.tmdb.org/p/exports/$FILENAME" | gunzip >"$EXPORT_NAME.json"
        env:
          FILENAME: ${{ steps.export_filename.outputs.yesterday_filename }}
          EXPORT_NAME: ${{ matrix.export_name }}

      - name: Build index
        shell: python
        run: |
          import os

          import numpy as np
          import pyarrow as pa
          import pyarrow.compute as pc
          import pyarrow.feather as feather
          from pyarrow import json

          export_name = os.environ["EXPORT_NAME"]
          table = json.read_json(f"{export_name}.json")
          size = pc.max(table["id"]).as_py() + 1

          adult = np.zeros(size, bool)
          adult[table["id"]] = table["adult"]

          mask = np.ones(size, bool)
          mask[table["id"]] = 0

          adult_col = pa.array(adult, type=pa.bool_(), mask=mask)
          table = pa.Table.from_arrays([adult_col], names=["adult"])
          feather.write_feather(table, "adult.arrow")
        env:
          EXPORT_NAME: ${{ matrix.export_name }}

      - name: Print stats
        run: |
          python print_table_stats.py adult.arrow

      - name: Upload artifacts
        uses: actions/upload-artifact@v3
        with:
          name: ${{ matrix.type }}-export
          path: |
            *.arrow
          retention-days: 3

      - name: Upload to S3
        run: |
          aws s3 cp "adult.arrow" "s3://$BUCKET_NAME/tmdb/$TYPE/adult.arrow"
        env:
          AWS_ACCESS_KEY_ID: ${{ secrets.AWS_ACCESS_KEY_ID }}
          AWS_SECRET_ACCESS_KEY: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          BUCKET_NAME: wikidatabots
          AWS_DEFAULT_REGION: us-east-1
          TYPE: ${{ matrix.type }}

  external_ids:
    needs: changed_at
    runs-on: ubuntu-latest
    concurrency: tmdb_external_ids_${{ matrix.type }}

    strategy:
      fail-fast: false
      matrix:
        include:
          - type: "movie"
          - type: "tv"
          - type: "person"

    steps:
      - uses: actions/checkout@v3

      - name: Set up Python 3.10
        uses: actions/setup-python@v4
        with:
          python-version: "3.10"
          cache: "pip"

      - name: Install dependencies
        run: |
          pip install -r requirements.txt

      - name: Download from S3
        run: |
          aws s3 cp "s3://$BUCKET_NAME/tmdb/$TYPE/changed_at.arrow" "changed_at.arrow"
          aws s3 cp "s3://$BUCKET_NAME/tmdb/$TYPE/external_ids.arrow" "external_ids.arrow"
        env:
          AWS_ACCESS_KEY_ID: ${{ secrets.AWS_ACCESS_KEY_ID }}
          AWS_SECRET_ACCESS_KEY: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          BUCKET_NAME: wikidatabots
          AWS_DEFAULT_REGION: us-east-1
          TYPE: ${{ matrix.type }}

      - name: Print stats
        shell: python
        run: |
          import os
          import time

          import numpy as np
          import pyarrow as pa
          import pyarrow.feather as feather

          start = time.time()
          table = feather.read_table("external_ids.arrow", memory_map=False)
          elapsed = time.time() - start

          imdb_ids = table["imdb_id"]
          size = len(imdb_ids)
          nonzero_count = np.count_nonzero(imdb_ids)
          zero_count = size - nonzero_count
          print(f"   imdb_id: {nonzero_count:,} ({nonzero_count/size:.2%})")
          print(f"no imdb_id: {zero_count:,} ({zero_count/size:.2%})")

          if os.environ["TYPE"] == "tv":
              tvdb_ids = table["tvdb_id"]
              size = len(tvdb_ids)
              nonzero_count = np.count_nonzero(tvdb_ids)
              zero_count = size - nonzero_count
              print("")
              print(f"   tvdb_id: {nonzero_count:,} ({nonzero_count/size:.2%})")
              print(f"no tvdb_id: {zero_count:,} ({zero_count/size:.2%})")

          timestamps = table["retrieved_at"]
          size = len(timestamps)
          nonzero_count = np.count_nonzero(~np.isnan(timestamps))
          zero_count = size - nonzero_count
          print("")
          print(f"   retrieved_at: {nonzero_count:,} ({nonzero_count/size:.2%})")
          print(f"no retrieved_at: {zero_count:,} ({zero_count/size:.2%})")

          print("")
          print(f"total: {size:,}")
          print(f" load: {elapsed:0.2}s")
          print(f"  rss: {pa.total_allocated_bytes() >> 20:,}MB")
        env:
          TYPE: ${{ matrix.type }}

      - name: Dry run next script
        continue-on-error: true
        run: |
          python tmdb_update_external_ids_next.py "$TYPE" "changed_at.arrow" "external_ids.arrow"
        env:
          TMDB_API_KEY: ${{ secrets.TMDB_API_KEY }}
          TYPE: ${{ matrix.type }}

      - name: Run script
        run: |
          python tmdb_update_external_ids.py "$TYPE" "external_ids.arrow"
        env:
          TMDB_API_KEY: ${{ secrets.TMDB_API_KEY }}
          TYPE: ${{ matrix.type }}

      - name: Print stats
        shell: python
        run: |
          import os
          import time

          import numpy as np
          import pyarrow as pa
          import pyarrow.feather as feather

          start = time.time()
          table = feather.read_table("external_ids.arrow", memory_map=False)
          elapsed = time.time() - start

          imdb_ids = table["imdb_id"]
          size = len(imdb_ids)
          nonzero_count = np.count_nonzero(imdb_ids)
          zero_count = size - nonzero_count
          print(f"   imdb_id: {nonzero_count:,} ({nonzero_count/size:.2%})")
          print(f"no imdb_id: {zero_count:,} ({zero_count/size:.2%})")

          if os.environ["TYPE"] == "tv":
              tvdb_ids = table["tvdb_id"]
              size = len(tvdb_ids)
              nonzero_count = np.count_nonzero(tvdb_ids)
              zero_count = size - nonzero_count
              print("")
              print(f"   tvdb_id: {nonzero_count:,} ({nonzero_count/size:.2%})")
              print(f"no tvdb_id: {zero_count:,} ({zero_count/size:.2%})")

          timestamps = table["retrieved_at"]
          size = len(timestamps)
          nonzero_count = np.count_nonzero(~np.isnan(timestamps))
          zero_count = size - nonzero_count
          print("")
          print(f"   retrieved_at: {nonzero_count:,} ({nonzero_count/size:.2%})")
          print(f"no retrieved_at: {zero_count:,} ({zero_count/size:.2%})")

          print("")
          print(f"total: {size:,}")
          print(f" load: {elapsed:0.2}s")
          print(f"  rss: {pa.total_allocated_bytes() >> 20:,}MB")
        env:
          TYPE: ${{ matrix.type }}

      - name: Upload artifacts
        uses: actions/upload-artifact@v3
        with:
          name: ${{ matrix.type }}-external_ids-export
          path: |
            *.arrow
          retention-days: 3

      - name: Upload to S3
        run: |
          aws s3 cp "external_ids.arrow" "s3://$BUCKET_NAME/tmdb/${TYPE}/external_ids.arrow"
        env:
          AWS_ACCESS_KEY_ID: ${{ secrets.AWS_ACCESS_KEY_ID }}
          AWS_SECRET_ACCESS_KEY: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          BUCKET_NAME: wikidatabots
          AWS_DEFAULT_REGION: us-east-1
          TYPE: ${{ matrix.type }}

  reverse_external_ids:
    needs: external_ids
    runs-on: ubuntu-latest
    concurrency: tmdb_reverse_external_ids_${{ matrix.type }}

    strategy:
      fail-fast: false
      matrix:
        include:
          - type: "movie"
            imdb_type: "tt"
          - type: "tv"
            imdb_type: "tt"
          - type: "person"
            imdb_type: "nm"

    steps:
      - uses: actions/checkout@v3

      - name: Set up Python 3.10
        uses: actions/setup-python@v4
        with:
          python-version: "3.10"
          cache: "pip"

      - name: Install dependencies
        run: |
          pip install -r requirements.txt

      - name: Download from S3
        run: |
          aws s3 cp "s3://$BUCKET_NAME/tmdb/$TYPE/external_ids.arrow" "external_ids.arrow"
        env:
          AWS_ACCESS_KEY_ID: ${{ secrets.AWS_ACCESS_KEY_ID }}
          AWS_SECRET_ACCESS_KEY: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          BUCKET_NAME: wikidatabots
          AWS_DEFAULT_REGION: us-east-1
          TYPE: ${{ matrix.type }}

      - name: Build reverse IMDB index
        shell: python
        run: |
          import numpy as np
          import pyarrow as pa
          import pyarrow.feather as feather

          table = feather.read_table("external_ids.arrow")
          valid_ids = table.column("imdb_id").to_numpy()
          bitmap = np.zeros(valid_ids.max() + 1, bool)
          bitmap[valid_ids] = 1
          table = pa.Table.from_arrays([bitmap], names=["tmdb_exists"])
          feather.write_feather(table, "imdb_tmdb_exists.arrow")

      - name: Print IMDB stats
        shell: python
        run: |
          import time

          import numpy as np
          import pyarrow as pa
          import pyarrow.feather as feather

          start = time.time()
          table = feather.read_table("imdb_tmdb_exists.arrow", memory_map=False)
          elapsed = time.time() - start

          tmdb_exists = table["tmdb_exists"]
          size = len(tmdb_exists)
          nonzero_count = np.count_nonzero(tmdb_exists)
          zero_count = size - nonzero_count
          print(f"   has tmdb: {nonzero_count:,} ({nonzero_count/size:.2%})")
          print(f"has no tmdb: {zero_count:,} ({zero_count/size:.2%})")
          print(f"      total: {size:,}")
          print(f"       load: {elapsed:0.2}s")
          print(f"        rss: {pa.total_allocated_bytes() >> 20:,}MB")

      - name: Build reverse TheTVDB index
        shell: python
        if: matrix.type == 'tv'
        run: |
          import numpy as np
          import pyarrow as pa
          import pyarrow.feather as feather

          table = feather.read_table("external_ids.arrow")
          valid_ids = table.column("tvdb_id").to_numpy()
          bitmap = np.zeros(valid_ids.max() + 1, bool)
          bitmap[valid_ids] = 1
          table = pa.Table.from_arrays([bitmap], names=["tmdb_exists"])
          feather.write_feather(table, "tvdb_tmdb_exists.arrow")

      - name: Print TheTVDB stats
        shell: python
        if: matrix.type == 'tv'
        run: |
          import time

          import numpy as np
          import pyarrow as pa
          import pyarrow.feather as feather

          start = time.time()
          table = feather.read_table("tvdb_tmdb_exists.arrow", memory_map=False)
          elapsed = time.time() - start

          tmdb_exists = table["tmdb_exists"]
          size = len(tmdb_exists)
          nonzero_count = np.count_nonzero(tmdb_exists)
          zero_count = size - nonzero_count
          print(f"   has tmdb: {nonzero_count:,} ({nonzero_count/size:.2%})")
          print(f"has no tmdb: {zero_count:,} ({zero_count/size:.2%})")
          print(f"      total: {size:,}")
          print(f"       load: {elapsed:0.2}s")
          print(f"        rss: {pa.total_allocated_bytes() >> 20:,}MB")

      - name: Upload artifacts
        uses: actions/upload-artifact@v3
        with:
          name: ${{ matrix.type }}-reverse_external_ids-export
          path: |
            *.arrow
          retention-days: 3

      - name: Upload to S3
        run: |
          aws s3 cp "imdb_tmdb_exists.arrow" "s3://$BUCKET_NAME/imdb/${IMDB_TYPE}/tmdb_${TYPE}_exists.arrow"
          if [ -f "tvdb_tmdb_exists.arrow" ]; then
            aws s3 cp "tvdb_tmdb_exists.arrow" "s3://$BUCKET_NAME/tvdb/tmdb_${TYPE}_exists.arrow"
          fi
        env:
          AWS_ACCESS_KEY_ID: ${{ secrets.AWS_ACCESS_KEY_ID }}
          AWS_SECRET_ACCESS_KEY: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          BUCKET_NAME: wikidatabots
          AWS_DEFAULT_REGION: us-east-1
          TYPE: ${{ matrix.type }}
          IMDB_TYPE: ${{ matrix.imdb_type }}

  changed_at:
    runs-on: ubuntu-latest
    concurrency: tmdb_changed_at_${{ matrix.type }}

    strategy:
      fail-fast: false
      matrix:
        include:
          - type: "movie"
          - type: "tv"
          - type: "person"

    steps:
      - uses: actions/checkout@v3

      - name: Set up Python 3.10
        uses: actions/setup-python@v4
        with:
          python-version: "3.10"
          cache: "pip"

      - name: Install dependencies
        run: |
          pip install -r requirements.txt

      - name: Download from S3
        run: |
          aws s3 cp "s3://$BUCKET_NAME/tmdb/$TYPE/changed_at.arrow" "changed_at.arrow"
        env:
          AWS_ACCESS_KEY_ID: ${{ secrets.AWS_ACCESS_KEY_ID }}
          AWS_SECRET_ACCESS_KEY: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          BUCKET_NAME: wikidatabots
          AWS_DEFAULT_REGION: us-east-1
          TYPE: ${{ matrix.type }}

      - name: Record changed at timestamps
        run: |
          python tmdb_update_changed_at.py "$TYPE" changed_at.arrow
        env:
          TMDB_API_KEY: ${{ secrets.TMDB_API_KEY }}
          TYPE: ${{ matrix.type }}

      - name: Print stats
        shell: python
        run: |
          import time

          import numpy as np
          import pyarrow as pa
          import pyarrow.feather as feather

          start = time.time()
          table = feather.read_table("changed_at.arrow", memory_map=False)
          elapsed = time.time() - start

          timestamps = table["changed_at"]
          size = len(timestamps)
          nonzero_count = np.count_nonzero(~np.isnan(timestamps))
          zero_count = size - nonzero_count
          print(f"   changed_at: {nonzero_count:,} ({nonzero_count/size:.2%})")
          print(f"no changed_at: {zero_count:,} ({zero_count/size:.2%})")
          print(f"        total: {size:,}")
          print(f"         load: {elapsed:0.2}s")
          print(f"          rss: {pa.total_allocated_bytes() >> 20:,}MB")

      - name: Upload artifacts
        uses: actions/upload-artifact@v3
        with:
          name: ${{ matrix.type }}-changed_at-export
          path: |
            *.arrow
          retention-days: 3

      - name: Upload to S3
        run: |
          aws s3 cp "changed_at.arrow" "s3://$BUCKET_NAME/tmdb/${TYPE}/changed_at.arrow"
        env:
          AWS_ACCESS_KEY_ID: ${{ secrets.AWS_ACCESS_KEY_ID }}
          AWS_SECRET_ACCESS_KEY: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          BUCKET_NAME: wikidatabots
          AWS_DEFAULT_REGION: us-east-1
          TYPE: ${{ matrix.type }}
