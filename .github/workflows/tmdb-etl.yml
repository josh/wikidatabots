name: TMDB ETL

on:
  schedule:
    - cron: "0 8 * * *"
  workflow_dispatch:

jobs:
  export:
    runs-on: ubuntu-latest
    concurrency: tmdb_export_${{ matrix.type }}

    strategy:
      fail-fast: false
      matrix:
        include:
          - type: "movie"
            export_name: "movie_ids"
          - type: "tv"
            export_name: "tv_series_ids"
          - type: "person"
            export_name: "person_ids"

    steps:
      - uses: actions/checkout@v3

      - name: Set up Python 3.10
        uses: actions/setup-python@v4
        with:
          python-version: "3.10"
          cache: "pip"

      - name: Install dependencies
        run: |
          pip install -r <(grep -E 'pandas|pyarrow' requirements.txt)

      - name: Generate export filename
        id: export_filename
        run: |
          TODAY=$(date --date=today +'%m_%d_%Y')
          YESTERDAY=$(date --date=yesterday +'%m_%d_%Y')
          echo "today_filename=${EXPORT_NAME}_${TODAY}.json.gz" >>"$GITHUB_OUTPUT"
          echo "yesterday_filename=${EXPORT_NAME}_${YESTERDAY}.json.gz" >>"$GITHUB_OUTPUT"
        env:
          EXPORT_NAME: ${{ matrix.export_name }}

      - name: Download today's export
        id: curl_today_export
        continue-on-error: true
        run: |
          curl --fail --no-progress-meter "http://files.tmdb.org/p/exports/$FILENAME" --output "export.json.gz"
        env:
          FILENAME: ${{ steps.export_filename.outputs.today_filename }}

      - name: Download yesterdays's export
        id: curl_yesterday_export
        if: steps.curl_today_export.outcome == 'failure'
        run: |
          curl --fail --no-progress-meter "http://files.tmdb.org/p/exports/$FILENAME" --output "export.json.gz"
        env:
          FILENAME: ${{ steps.export_filename.outputs.yesterday_filename }}

      - name: Transform export data
        shell: python
        run: |
          import pandas as pd

          df = pd.read_json("export.json.gz", lines=True).set_index("id")
          df["in_export"] = True
          size = df.index.max() + 1
          df = df.reindex(pd.RangeIndex(0, size))
          df["in_export"] = df["in_export"].fillna(False)
          df.reset_index(names=["id"]).to_feather("export.arrow")

      - name: Print stats
        run: |
          python print_table_stats.py export.arrow

      - name: Upload artifacts
        uses: actions/upload-artifact@v3
        with:
          name: ${{ matrix.type }}-export
          path: |
            *.arrow
          retention-days: 3

      - name: Upload to S3
        run: |
          aws s3 cp "export.arrow" "s3://$BUCKET_NAME/tmdb/$TYPE/export.arrow"
        env:
          AWS_ACCESS_KEY_ID: ${{ secrets.AWS_ACCESS_KEY_ID }}
          AWS_SECRET_ACCESS_KEY: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          BUCKET_NAME: wikidatabots
          AWS_DEFAULT_REGION: us-east-1
          TYPE: ${{ matrix.type }}

  external_ids:
    needs: changes
    runs-on: ubuntu-latest
    concurrency: tmdb_external_ids_${{ matrix.type }}

    strategy:
      fail-fast: false
      matrix:
        include:
          - type: "movie"
          - type: "tv"
          - type: "person"

    steps:
      - uses: actions/checkout@v3

      - name: Set up Python 3.10
        uses: actions/setup-python@v4
        with:
          python-version: "3.10"
          cache: "pip"

      - name: Install dependencies
        run: |
          pip install -r requirements.txt

      - name: Download from S3
        run: |
          aws s3 cp "s3://$BUCKET_NAME/tmdb/$TYPE/changes.arrow" "changes.arrow"
          aws s3 cp "s3://$BUCKET_NAME/tmdb/$TYPE/external_ids.arrow" "external_ids.arrow"
        env:
          AWS_ACCESS_KEY_ID: ${{ secrets.AWS_ACCESS_KEY_ID }}
          AWS_SECRET_ACCESS_KEY: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          BUCKET_NAME: wikidatabots
          AWS_DEFAULT_REGION: us-east-1
          TYPE: ${{ matrix.type }}

      - name: Print stats
        run: |
          python print_table_stats.py external_ids.arrow

      - name: Get changed IDs
        id: tmdb_external_ids_outdated
        run: |
          python tmdb_external_ids_outdated.py external_ids.arrow changes.arrow >"$RUNNER_TEMP/changed_ids.txt"
          count=$(wc -l <"$RUNNER_TEMP/changed_ids.txt")
          echo "count=$count" | tee --append "$GITHUB_OUTPUT"

      - name: Generate changed curl config
        id: curl_config
        shell: bash
        run: |
          curl_config() {
            while read -r ID; do
              URL="https://api.themoviedb.org/3/$TYPE/$ID/external_ids?api_key=$TMDB_API_KEY"
              echo "url = \"$URL\""
              echo "output = \"$RUNNER_TEMP/external_ids/$ID.json\""
            done
          }
          mkdir -p "$RUNNER_TEMP/external_ids/"
          curl_config <"$RUNNER_TEMP/changed_ids.txt" | tee "$RUNNER_TEMP/curl_config.txt"
        env:
          TMDB_API_KEY: ${{ secrets.TMDB_API_KEY }}
          TYPE: ${{ matrix.type }}

      - name: Fetch changed TMDB external IDs
        if: steps.tmdb_external_ids_outdated.outputs.count > 0
        run: |
          curl --config "$RUNNER_TEMP/curl_config.txt"

      - name: Run script
        run: |
          python tmdb_update_external_ids.py "$TYPE" "external_ids.arrow" "$RUNNER_TEMP/external_ids"
        env:
          TMDB_API_KEY: ${{ secrets.TMDB_API_KEY }}
          TYPE: ${{ matrix.type }}

      - name: Print stats
        run: |
          python print_table_stats.py external_ids.arrow

      - name: Upload artifacts
        uses: actions/upload-artifact@v3
        with:
          name: ${{ matrix.type }}-external_ids-export
          path: |
            *.arrow
          retention-days: 3

      - name: Upload to S3
        run: |
          aws s3 cp "external_ids.arrow" "s3://$BUCKET_NAME/tmdb/${TYPE}/external_ids.arrow"
        env:
          AWS_ACCESS_KEY_ID: ${{ secrets.AWS_ACCESS_KEY_ID }}
          AWS_SECRET_ACCESS_KEY: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          BUCKET_NAME: wikidatabots
          AWS_DEFAULT_REGION: us-east-1
          TYPE: ${{ matrix.type }}

  reverse_external_ids:
    needs: external_ids
    runs-on: ubuntu-latest
    concurrency: tmdb_reverse_external_ids_${{ matrix.type }}

    strategy:
      fail-fast: false
      matrix:
        include:
          - type: "movie"
            imdb_type: "tt"
          - type: "tv"
            imdb_type: "tt"
          - type: "person"
            imdb_type: "nm"

    steps:
      - uses: actions/checkout@v3

      - name: Set up Python 3.10
        uses: actions/setup-python@v4
        with:
          python-version: "3.10"
          cache: "pip"

      - name: Install dependencies
        run: |
          pip install -r <(grep -E 'numpy|pyarrow' requirements.txt)

      - name: Download from S3
        run: |
          aws s3 cp "s3://$BUCKET_NAME/tmdb/$TYPE/external_ids.arrow" "external_ids.arrow"
        env:
          AWS_ACCESS_KEY_ID: ${{ secrets.AWS_ACCESS_KEY_ID }}
          AWS_SECRET_ACCESS_KEY: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          BUCKET_NAME: wikidatabots
          AWS_DEFAULT_REGION: us-east-1
          TYPE: ${{ matrix.type }}

      - name: Build reverse IMDB index
        shell: python
        run: |
          import numpy as np
          import pyarrow as pa
          import pyarrow.feather as feather

          table = feather.read_table("external_ids.arrow")
          valid_ids = table.column("imdb_id").to_numpy()
          bitmap = np.zeros(valid_ids.max() + 1, bool)
          bitmap[valid_ids] = 1
          table = pa.Table.from_arrays([bitmap], names=["tmdb_exists"])
          feather.write_feather(table, "imdb_tmdb_exists.arrow")

      - name: Print IMDB stats
        run: |
          python print_table_stats.py imdb_tmdb_exists.arrow

      - name: Build reverse TheTVDB index
        shell: python
        if: matrix.type == 'tv'
        run: |
          import numpy as np
          import pyarrow as pa
          import pyarrow.feather as feather

          table = feather.read_table("external_ids.arrow")
          valid_ids = table.column("tvdb_id").to_numpy()
          bitmap = np.zeros(valid_ids.max() + 1, bool)
          bitmap[valid_ids] = 1
          table = pa.Table.from_arrays([bitmap], names=["tmdb_exists"])
          feather.write_feather(table, "tvdb_tmdb_exists.arrow")

      - name: Print TheTVDB stats
        if: matrix.type == 'tv'
        run: |
          python print_table_stats.py tvdb_tmdb_exists.arrow

      - name: Upload artifacts
        uses: actions/upload-artifact@v3
        with:
          name: ${{ matrix.type }}-reverse_external_ids-export
          path: |
            *.arrow
          retention-days: 3

      - name: Upload to S3
        run: |
          aws s3 cp "imdb_tmdb_exists.arrow" "s3://$BUCKET_NAME/imdb/${IMDB_TYPE}/tmdb_${TYPE}_exists.arrow"
          if [ -f "tvdb_tmdb_exists.arrow" ]; then
            aws s3 cp "tvdb_tmdb_exists.arrow" "s3://$BUCKET_NAME/tvdb/tmdb_${TYPE}_exists.arrow"
          fi
        env:
          AWS_ACCESS_KEY_ID: ${{ secrets.AWS_ACCESS_KEY_ID }}
          AWS_SECRET_ACCESS_KEY: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          BUCKET_NAME: wikidatabots
          AWS_DEFAULT_REGION: us-east-1
          TYPE: ${{ matrix.type }}
          IMDB_TYPE: ${{ matrix.imdb_type }}

  changes:
    runs-on: ubuntu-latest
    concurrency: tmdb_changes_${{ matrix.type }}

    strategy:
      fail-fast: false
      matrix:
        include:
          - type: "movie"
          - type: "tv"
          - type: "person"

    steps:
      - uses: actions/checkout@v3

      - name: Set up Python 3.10
        uses: actions/setup-python@v4
        with:
          python-version: "3.10"
          cache: "pip"

      - name: Install dependencies
        run: |
          pip install -r <(grep -E 'pandas|pyarrow' requirements.txt)

      - name: Download from S3
        run: |
          aws s3 cp "s3://$BUCKET_NAME/tmdb/$TYPE/changes.arrow" "changes.arrow"
        env:
          AWS_ACCESS_KEY_ID: ${{ secrets.AWS_ACCESS_KEY_ID }}
          AWS_SECRET_ACCESS_KEY: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          BUCKET_NAME: wikidatabots
          AWS_DEFAULT_REGION: us-east-1
          TYPE: ${{ matrix.type }}

      - name: Fetch recent TMDB changes
        shell: bash
        run: |
          curl_config() {
            for N in $(seq 0 6); do
              START_DATE=$(date --date="today -$N day" +%Y-%m-%d)
              END_DATE=$(date --date="today -$N day +1 day" +%Y-%m-%d)
              URL="https://api.themoviedb.org/3/$TYPE/changes?api_key=$TMDB_API_KEY&start_date=$START_DATE&end_date=$END_DATE"
              echo "url = \"$URL\""
              echo "output = \"$RUNNER_TEMP/changes/$START_DATE.json\""
            done
          }
          mkdir -p "$RUNNER_TEMP/changes/"
          curl_config | tee "$RUNNER_TEMP/curl_config.txt"
          curl --fail --config "$RUNNER_TEMP/curl_config.txt"
        env:
          TMDB_API_KEY: ${{ secrets.TMDB_API_KEY }}
          TYPE: ${{ matrix.type }}

      - name: Append TMDB changes
        run: |
          python tmdb_append_changes.py changes.arrow "$RUNNER_TEMP/changes"

      - name: Print stats
        run: |
          python print_table_stats.py changes.arrow

      - name: Upload artifacts
        uses: actions/upload-artifact@v3
        with:
          name: ${{ matrix.type }}-changes-export
          path: |
            *.arrow
          retention-days: 3

      - name: Upload to S3
        run: |
          aws s3 cp "changes.arrow" "s3://$BUCKET_NAME/tmdb/${TYPE}/changes.arrow"
        env:
          AWS_ACCESS_KEY_ID: ${{ secrets.AWS_ACCESS_KEY_ID }}
          AWS_SECRET_ACCESS_KEY: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          BUCKET_NAME: wikidatabots
          AWS_DEFAULT_REGION: us-east-1
          TYPE: ${{ matrix.type }}
