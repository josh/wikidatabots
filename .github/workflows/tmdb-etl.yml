name: TMDB ETL

on:
  schedule:
    - cron: "0 8 * * *"
  workflow_dispatch:

jobs:
  export:
    runs-on: ubuntu-latest
    concurrency: tmdb_export_${{ matrix.type }}

    strategy:
      fail-fast: false
      matrix:
        include:
          - type: "movie"
            export_name: "movie_ids"
          - type: "tv"
            export_name: "tv_series_ids"
          - type: "person"
            export_name: "person_ids"

    steps:
      - uses: actions/checkout@v3

      - name: Set up Python 3.10
        uses: actions/setup-python@v4
        with:
          python-version: "3.10"
          cache: "pip"

      - name: Install dependencies
        run: |
          pip install -r <(grep -E 'pandas|pyarrow' requirements.txt)

      - name: Generate export filename
        id: export_filename
        run: |
          TODAY=$(date --date=today +'%m_%d_%Y')
          YESTERDAY=$(date --date=yesterday +'%m_%d_%Y')
          echo "today_filename=${EXPORT_NAME}_${TODAY}.json.gz" >>"$GITHUB_OUTPUT"
          echo "yesterday_filename=${EXPORT_NAME}_${YESTERDAY}.json.gz" >>"$GITHUB_OUTPUT"
        env:
          EXPORT_NAME: ${{ matrix.export_name }}

      - name: Download today's export
        id: curl_today_export
        continue-on-error: true
        run: |
          curl --fail --no-progress-meter "http://files.tmdb.org/p/exports/$FILENAME" --output "export.json.gz"
        env:
          FILENAME: ${{ steps.export_filename.outputs.today_filename }}

      - name: Download yesterdays's export
        id: curl_yesterday_export
        if: steps.curl_today_export.outcome == 'failure'
        run: |
          curl --fail --no-progress-meter "http://files.tmdb.org/p/exports/$FILENAME" --output "export.json.gz"
        env:
          FILENAME: ${{ steps.export_filename.outputs.yesterday_filename }}

      - name: Transform export data
        shell: python
        run: |
          import pandas as pd

          df = pd.read_json("export.json.gz", lines=True).set_index("id")
          df["in_export"] = True

          size = df.index.max() + 2
          df2 = pd.DataFrame({"id": range(size)}).set_index("id")

          df = df.join(df2, how="right", sort=True)
          df["in_export"] = df["in_export"].fillna(False)
          df.reset_index().to_feather("export.arrow")

      - name: Print stats
        run: |
          python print_table_stats.py export.arrow

      - name: Upload artifacts
        uses: actions/upload-artifact@v3
        with:
          name: ${{ matrix.type }}-export
          path: |
            *.arrow
          retention-days: 3

      - name: Upload to S3
        run: |
          aws s3 cp "export.arrow" "s3://$BUCKET_NAME/tmdb/$TYPE/export.arrow"
        env:
          AWS_ACCESS_KEY_ID: ${{ secrets.AWS_ACCESS_KEY_ID }}
          AWS_SECRET_ACCESS_KEY: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          BUCKET_NAME: wikidatabots
          AWS_DEFAULT_REGION: us-east-1
          TYPE: ${{ matrix.type }}

  external_ids:
    needs: changed_at
    runs-on: ubuntu-latest
    concurrency: tmdb_external_ids_${{ matrix.type }}

    strategy:
      fail-fast: false
      matrix:
        include:
          - type: "movie"
          - type: "tv"
          - type: "person"

    steps:
      - uses: actions/checkout@v3

      - name: Set up Python 3.10
        uses: actions/setup-python@v4
        with:
          python-version: "3.10"
          cache: "pip"

      - name: Install dependencies
        run: |
          pip install -r requirements.txt

      - name: Download from S3
        run: |
          aws s3 cp "s3://$BUCKET_NAME/tmdb/$TYPE/changed_at.arrow" "changed_at.arrow"
          aws s3 cp "s3://$BUCKET_NAME/tmdb/$TYPE/external_ids.arrow" "external_ids.arrow"
        env:
          AWS_ACCESS_KEY_ID: ${{ secrets.AWS_ACCESS_KEY_ID }}
          AWS_SECRET_ACCESS_KEY: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          BUCKET_NAME: wikidatabots
          AWS_DEFAULT_REGION: us-east-1
          TYPE: ${{ matrix.type }}

      - name: Print stats
        run: |
          python print_table_stats.py external_ids.arrow

      - name: Dry run next script
        continue-on-error: true
        run: |
          python tmdb_update_external_ids_next.py "$TYPE" "changed_at.arrow" "external_ids.arrow"
        env:
          TMDB_API_KEY: ${{ secrets.TMDB_API_KEY }}
          TYPE: ${{ matrix.type }}

      - name: Run script
        run: |
          python tmdb_update_external_ids.py "$TYPE" "external_ids.arrow"
        env:
          TMDB_API_KEY: ${{ secrets.TMDB_API_KEY }}
          TYPE: ${{ matrix.type }}

      - name: Print stats
        run: |
          python print_table_stats.py external_ids.arrow

      - name: Upload artifacts
        uses: actions/upload-artifact@v3
        with:
          name: ${{ matrix.type }}-external_ids-export
          path: |
            *.arrow
          retention-days: 3

      - name: Upload to S3
        run: |
          aws s3 cp "external_ids.arrow" "s3://$BUCKET_NAME/tmdb/${TYPE}/external_ids.arrow"
        env:
          AWS_ACCESS_KEY_ID: ${{ secrets.AWS_ACCESS_KEY_ID }}
          AWS_SECRET_ACCESS_KEY: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          BUCKET_NAME: wikidatabots
          AWS_DEFAULT_REGION: us-east-1
          TYPE: ${{ matrix.type }}

  reverse_external_ids:
    needs: external_ids
    runs-on: ubuntu-latest
    concurrency: tmdb_reverse_external_ids_${{ matrix.type }}

    strategy:
      fail-fast: false
      matrix:
        include:
          - type: "movie"
            imdb_type: "tt"
          - type: "tv"
            imdb_type: "tt"
          - type: "person"
            imdb_type: "nm"

    steps:
      - uses: actions/checkout@v3

      - name: Set up Python 3.10
        uses: actions/setup-python@v4
        with:
          python-version: "3.10"
          cache: "pip"

      - name: Install dependencies
        run: |
          pip install -r requirements.txt

      - name: Download from S3
        run: |
          aws s3 cp "s3://$BUCKET_NAME/tmdb/$TYPE/external_ids.arrow" "external_ids.arrow"
        env:
          AWS_ACCESS_KEY_ID: ${{ secrets.AWS_ACCESS_KEY_ID }}
          AWS_SECRET_ACCESS_KEY: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          BUCKET_NAME: wikidatabots
          AWS_DEFAULT_REGION: us-east-1
          TYPE: ${{ matrix.type }}

      - name: Build reverse IMDB index
        shell: python
        run: |
          import numpy as np
          import pyarrow as pa
          import pyarrow.feather as feather

          table = feather.read_table("external_ids.arrow")
          valid_ids = table.column("imdb_id").to_numpy()
          bitmap = np.zeros(valid_ids.max() + 1, bool)
          bitmap[valid_ids] = 1
          table = pa.Table.from_arrays([bitmap], names=["tmdb_exists"])
          feather.write_feather(table, "imdb_tmdb_exists.arrow")

      - name: Print IMDB stats
        run: |
          python print_table_stats.py imdb_tmdb_exists.arrow

      - name: Build reverse TheTVDB index
        shell: python
        if: matrix.type == 'tv'
        run: |
          import numpy as np
          import pyarrow as pa
          import pyarrow.feather as feather

          table = feather.read_table("external_ids.arrow")
          valid_ids = table.column("tvdb_id").to_numpy()
          bitmap = np.zeros(valid_ids.max() + 1, bool)
          bitmap[valid_ids] = 1
          table = pa.Table.from_arrays([bitmap], names=["tmdb_exists"])
          feather.write_feather(table, "tvdb_tmdb_exists.arrow")

      - name: Print TheTVDB stats
        if: matrix.type == 'tv'
        run: |
          python print_table_stats.py tvdb_tmdb_exists.arrow

      - name: Upload artifacts
        uses: actions/upload-artifact@v3
        with:
          name: ${{ matrix.type }}-reverse_external_ids-export
          path: |
            *.arrow
          retention-days: 3

      - name: Upload to S3
        run: |
          aws s3 cp "imdb_tmdb_exists.arrow" "s3://$BUCKET_NAME/imdb/${IMDB_TYPE}/tmdb_${TYPE}_exists.arrow"
          if [ -f "tvdb_tmdb_exists.arrow" ]; then
            aws s3 cp "tvdb_tmdb_exists.arrow" "s3://$BUCKET_NAME/tvdb/tmdb_${TYPE}_exists.arrow"
          fi
        env:
          AWS_ACCESS_KEY_ID: ${{ secrets.AWS_ACCESS_KEY_ID }}
          AWS_SECRET_ACCESS_KEY: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          BUCKET_NAME: wikidatabots
          AWS_DEFAULT_REGION: us-east-1
          TYPE: ${{ matrix.type }}
          IMDB_TYPE: ${{ matrix.imdb_type }}

  changed_at:
    runs-on: ubuntu-latest
    concurrency: tmdb_changed_at_${{ matrix.type }}

    strategy:
      fail-fast: false
      matrix:
        include:
          - type: "movie"
          - type: "tv"
          - type: "person"

    steps:
      - uses: actions/checkout@v3

      - name: Set up Python 3.10
        uses: actions/setup-python@v4
        with:
          python-version: "3.10"
          cache: "pip"

      - name: Install dependencies
        run: |
          pip install -r requirements.txt

      - name: Download from S3
        run: |
          aws s3 cp "s3://$BUCKET_NAME/tmdb/$TYPE/changes.arrow" "changes.arrow"
          aws s3 cp "s3://$BUCKET_NAME/tmdb/$TYPE/changed_at.arrow" "changed_at.arrow"
        env:
          AWS_ACCESS_KEY_ID: ${{ secrets.AWS_ACCESS_KEY_ID }}
          AWS_SECRET_ACCESS_KEY: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          BUCKET_NAME: wikidatabots
          AWS_DEFAULT_REGION: us-east-1
          TYPE: ${{ matrix.type }}

      - name: Fetch recent TMDB changes
        shell: bash
        run: |
          mkdir -p "$RUNNER_TEMP/changes/"
          for N in $(seq 0 6); do
            START_DATE=$(date --date="today -$N day" +%Y-%m-%d)
            END_DATE=$(date --date="today -$N day +1 day" +%Y-%m-%d)
            URL="https://api.themoviedb.org/3/$TYPE/changes?api_key=$TMDB_API_KEY&start_date=$START_DATE&end_date=$END_DATE"
            echo "curl $URL" >/dev/stderr
            curl --no-progress-meter "$URL" | jq --compact-output '.results[]' | python json_to_arrow.py >"$RUNNER_TEMP/changes/$START_DATE.arrow"
            python print_table_stats.py "$RUNNER_TEMP/changes/$START_DATE.arrow"
            python tmdb_append_changes.py changes.arrow "$RUNNER_TEMP/changes/$START_DATE.arrow"
          done
        env:
          TMDB_API_KEY: ${{ secrets.TMDB_API_KEY }}
          TYPE: ${{ matrix.type }}

      - name: Record changed at timestamps
        run: |
          for filename in "$RUNNER_TEMP/changes"/*.arrow; do
            python tmdb_update_changed_at.py changed_at.arrow "$filename"
          done

      - name: Print stats
        run: |
          python print_table_stats.py changes.arrow
          python print_table_stats.py changed_at.arrow

      - name: Upload artifacts
        uses: actions/upload-artifact@v3
        with:
          name: ${{ matrix.type }}-changed_at-export
          path: |
            *.arrow
          retention-days: 3

      - name: Upload to S3
        run: |
          aws s3 cp "changes.arrow" "s3://$BUCKET_NAME/tmdb/${TYPE}/changes.arrow"
          aws s3 cp "changed_at.arrow" "s3://$BUCKET_NAME/tmdb/${TYPE}/changed_at.arrow"
        env:
          AWS_ACCESS_KEY_ID: ${{ secrets.AWS_ACCESS_KEY_ID }}
          AWS_SECRET_ACCESS_KEY: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          BUCKET_NAME: wikidatabots
          AWS_DEFAULT_REGION: us-east-1
          TYPE: ${{ matrix.type }}
