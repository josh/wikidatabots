name: Apple TV ETL

on:
  schedule:
    - cron: "0 16 * * 1"
  workflow_dispatch:

permissions:
  id-token: write
  contents: read

jobs:
  sitemap:
    runs-on: ubuntu-latest

    strategy:
      fail-fast: false
      matrix:
        include:
          - type: "episode"
          - type: "movie"
          - type: "show"

    steps:
      - uses: actions/checkout@v3

      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v1-node16
        with:
          role-to-assume: ${{ secrets.AWS_ROLE_TO_ASSUME }}
          aws-region: us-east-1

      - name: Set up Python 3.10
        uses: actions/setup-python@v4
        with:
          python-version: "3.10"
          cache: "pip"

      - name: Install dependencies
        run: |
          pip install -r <(grep -E 'fsspec|lxml|pandas|pyarrow' requirements.txt)

      - name: Download sitemap index
        run: |
          curl --fail --no-progress-meter "https://tv.apple.com/sitemaps_tv_index_${TYPE}_1.xml" --output "sitemap_index.xml"
        env:
          TYPE: ${{ matrix.type }}

      - name: Fetch sitemaps
        shell: python
        run: |
          import pandas as pd

          index_df = pd.read_xml("sitemap_index.xml")
          dtype = {
              "loc": "string",
              "lastmod": "datetime64[ns]",
              "changefreq": "category",
              "priority": "float16",
              "link": "string",
          }
          dfs = [pd.read_xml(url, dtype=dtype, parser="etree") for url in index_df["loc"]]
          df = pd.concat(dfs, ignore_index=True)
          df.to_feather("sitemap_changes.arrow")

      - name: Clean up sitemap
        shell: python
        run: |
          import urllib.parse

          import pandas as pd

          df = pd.read_feather("sitemap_changes.arrow")
          df = df.drop(columns=["link"])
          loc_df = (
              df["loc"]
              .str.extract(
                  r"https://tv.apple.com/"
                  r"(?P<country>[a-z]{2})/"
                  r"(?P<type>episode|movie|show)/"
                  r"(?P<slug>[^/]*)/"
                  r"(?P<id>umc.cmc.[0-9a-z]+)"
              )
              .astype({"country": "category", "type": "category"})
          )
          loc_df["slug"] = loc_df["slug"].apply(urllib.parse.unquote).astype("string")
          df = pd.concat([df, loc_df], axis=1)
          df["in_latest_sitemap"] = True
          df.to_feather("sitemap_changes.arrow")

      - name: Print stats
        run: |
          python print_table_stats.py sitemap_changes.arrow

      - name: Download from S3
        run: |
          aws s3 cp "s3://$BUCKET_NAME/appletv/$TYPE/sitemap.arrow" "sitemap.arrow"
          cp sitemap.arrow sitemap.arrow~
        env:
          BUCKET_NAME: wikidatabots
          TYPE: ${{ matrix.type }}

      - name: Join sitemap changes
        shell: python
        run: |
          import pandas as pd

          df = pd.read_feather("sitemap.arrow")
          df["in_latest_sitemap"] = False
          df2 = pd.read_feather("sitemap_changes.arrow")
          existing_changes = df["loc"].isin(df2["loc"])
          df = pd.concat([df[~existing_changes], df2], ignore_index=True)
          df.to_feather("sitemap.arrow")

      - name: Print stats
        run: |
          python print_table_stats.py sitemap.arrow

      - name: Print diff
        run: |
          python print_table_diff.py sitemap.arrow~ sitemap.arrow loc

      - name: Upload artifacts
        uses: actions/upload-artifact@v3
        with:
          name: ${{ matrix.type }}-sitemap
          path: |
            *.arrow
          retention-days: 30

      - name: Upload to S3
        run: |
          aws s3 cp "sitemap.arrow" "s3://$BUCKET_NAME/appletv/$TYPE/sitemap.arrow"
        env:
          BUCKET_NAME: wikidatabots
          TYPE: ${{ matrix.type }}
